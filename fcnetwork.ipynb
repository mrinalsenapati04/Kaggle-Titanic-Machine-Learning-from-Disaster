{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb3d3b1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:14.659552Z",
     "iopub.status.busy": "2022-05-30T14:32:14.659104Z",
     "iopub.status.idle": "2022-05-30T14:32:17.652125Z",
     "shell.execute_reply": "2022-05-30T14:32:17.651042Z"
    },
    "papermill": {
     "duration": 3.00305,
     "end_time": "2022-05-30T14:32:17.654834",
     "exception": false,
     "start_time": "2022-05-30T14:32:14.651784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596, 6) (295, 6) (596, 1) (295, 1)\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "def read_data():    \n",
    "    dataset = {}\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            data_path = os.path.join(dirname, filename)\n",
    "            if filename.split('.')[0] == 'train':\n",
    "                dataset['train']= pd.read_csv(data_path)\n",
    "            if filename.split('.')[0] == 'test':\n",
    "                dataset['test'] = pd.read_csv(data_path)\n",
    "            else:\n",
    "                dataset['sample'] = pd.read_csv(data_path)\n",
    "    return dataset\n",
    "\n",
    "def data_preprocessing(dataset):         \n",
    "    # print(dataset['train'].head)\n",
    "    # sns.countplot(x='Survived', data = dataset['train'])\n",
    "    # print(dataset['train'].shape)\n",
    "    dataset.drop(['PassengerId','Name','Ticket','Cabin','Embarked'], axis = 1, inplace = True)\n",
    "    dataset['Sex'].replace({'male':0, 'female':1}, inplace = True)\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "#     print(dataset.isna().any())\n",
    "    \n",
    "    \n",
    "    X = dataset.loc[:,dataset.columns != 'Survived']\n",
    "    # print(X.shape)\n",
    "    y = dataset.loc[:,dataset.columns == 'Survived']\n",
    "    # print(y.shape)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.33, random_state = 69)\n",
    "#     for data in X_train:\n",
    "#         print(X_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    y_train = y_train.to_numpy()\n",
    "    t_test = y_test.to_numpy()\n",
    "    # print(X_train)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "dataset = read_data()\n",
    "(X_train, X_test,y_train, y_test) = data_preprocessing(dataset['train'])\n",
    "# data_preprocessing(dataset['train'])\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb13df6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:17.666777Z",
     "iopub.status.busy": "2022-05-30T14:32:17.666384Z",
     "iopub.status.idle": "2022-05-30T14:32:17.680347Z",
     "shell.execute_reply": "2022-05-30T14:32:17.679420Z"
    },
    "papermill": {
     "duration": 0.022698,
     "end_time": "2022-05-30T14:32:17.682770",
     "exception": false,
     "start_time": "2022-05-30T14:32:17.660072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596, 6) (295, 6) (596, 1) (295, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f076dd80e10> <torch.utils.data.dataloader.DataLoader object at 0x7f076dd74350>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "class TestData(Dataset):\n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "train_data = TrainData(torch.FloatTensor(X_train), \n",
    "                      torch.FloatTensor(y_train))\n",
    "test_data = TestData(torch.FloatTensor(X_test))\n",
    "\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = 1)\n",
    "\n",
    "# for X,y in train_loader:\n",
    "#     print (X)\n",
    "\n",
    "print(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55f12ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:17.694472Z",
     "iopub.status.busy": "2022-05-30T14:32:17.694083Z",
     "iopub.status.idle": "2022-05-30T14:32:17.704176Z",
     "shell.execute_reply": "2022-05-30T14:32:17.703035Z"
    },
    "papermill": {
     "duration": 0.018583,
     "end_time": "2022-05-30T14:32:17.706354",
     "exception": false,
     "start_time": "2022-05-30T14:32:17.687771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hid_dims):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dims = hid_dims\n",
    "        current_dim = in_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "        for hdim in hid_dims:\n",
    "            self.layers.append(nn.Linear(current_dim, hdim))\n",
    "            self.layers.append(nn.BatchNorm1d(hdim))\n",
    "            current_dim = hdim\n",
    "        self.layers.append(nn.Linear(current_dim, out_dim))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "#         self.batchnorms = []\n",
    "#         for hdim in hid_dims:\n",
    "#             self.batchnorms.append(nn.BatchNorm1d(hdim))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        idx = 0\n",
    "        x = inputs\n",
    "        for layer in self.layers[:-1]:\n",
    "#             print ('input ', x)\n",
    "            x = self.relu(layer(x))\n",
    "#             x= self.batchnorms[idx](x)\n",
    "            idx =idx + 1\n",
    "#             print('output ',x)\n",
    "#         print ('input', x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layers[-1](x)\n",
    "#         print ('output', x)\n",
    "        return x\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f856c3c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:17.717848Z",
     "iopub.status.busy": "2022-05-30T14:32:17.717465Z",
     "iopub.status.idle": "2022-05-30T14:32:17.730308Z",
     "shell.execute_reply": "2022-05-30T14:32:17.728954Z"
    },
    "papermill": {
     "duration": 0.02175,
     "end_time": "2022-05-30T14:32:17.733172",
     "exception": false,
     "start_time": "2022-05-30T14:32:17.711422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "BinaryClassification(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=6, out_features=12, bias=True)\n",
      "    (1): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (3): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (5): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = BinaryClassification(6, 1, [12,12,12])\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a5cd560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:17.745753Z",
     "iopub.status.busy": "2022-05-30T14:32:17.745333Z",
     "iopub.status.idle": "2022-05-30T14:32:18.868309Z",
     "shell.execute_reply": "2022-05-30T14:32:18.867344Z"
    },
    "papermill": {
     "duration": 1.131923,
     "end_time": "2022-05-30T14:32:18.870763",
     "exception": false,
     "start_time": "2022-05-30T14:32:17.738840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.68769 | Acc: 60.300\n",
      "Epoch 002: | Loss: 0.66294 | Acc: 62.700\n",
      "Epoch 003: | Loss: 0.64589 | Acc: 65.400\n",
      "Epoch 004: | Loss: 0.63675 | Acc: 65.800\n",
      "Epoch 005: | Loss: 0.61726 | Acc: 71.000\n",
      "Epoch 006: | Loss: 0.61386 | Acc: 68.600\n",
      "Epoch 007: | Loss: 0.60485 | Acc: 70.600\n",
      "Epoch 008: | Loss: 0.59611 | Acc: 72.700\n",
      "Epoch 009: | Loss: 0.59782 | Acc: 71.500\n",
      "Epoch 010: | Loss: 0.57521 | Acc: 75.800\n",
      "Epoch 011: | Loss: 0.57268 | Acc: 75.500\n",
      "Epoch 012: | Loss: 0.56820 | Acc: 75.200\n",
      "Epoch 013: | Loss: 0.56059 | Acc: 76.200\n",
      "Epoch 014: | Loss: 0.53901 | Acc: 77.100\n",
      "Epoch 015: | Loss: 0.54167 | Acc: 77.900\n",
      "Epoch 016: | Loss: 0.53077 | Acc: 77.200\n",
      "Epoch 017: | Loss: 0.52756 | Acc: 77.900\n",
      "Epoch 018: | Loss: 0.51127 | Acc: 79.600\n",
      "Epoch 019: | Loss: 0.50566 | Acc: 78.900\n",
      "Epoch 020: | Loss: 0.51538 | Acc: 79.500\n",
      "Epoch 021: | Loss: 0.50031 | Acc: 79.100\n",
      "Epoch 022: | Loss: 0.49372 | Acc: 79.900\n",
      "Epoch 023: | Loss: 0.47077 | Acc: 82.700\n",
      "Epoch 024: | Loss: 0.47995 | Acc: 81.600\n",
      "Epoch 025: | Loss: 0.46619 | Acc: 81.200\n",
      "Epoch 026: | Loss: 0.45295 | Acc: 82.000\n",
      "Epoch 027: | Loss: 0.43901 | Acc: 82.000\n",
      "Epoch 028: | Loss: 0.44585 | Acc: 82.400\n",
      "Epoch 029: | Loss: 0.45950 | Acc: 80.200\n",
      "Epoch 030: | Loss: 0.44614 | Acc: 81.400\n",
      "Epoch 031: | Loss: 0.46324 | Acc: 81.300\n",
      "Epoch 032: | Loss: 0.44099 | Acc: 81.500\n",
      "Epoch 033: | Loss: 0.43510 | Acc: 82.000\n",
      "Epoch 034: | Loss: 0.43977 | Acc: 82.600\n",
      "Epoch 035: | Loss: 0.42917 | Acc: 82.900\n",
      "Epoch 036: | Loss: 0.42057 | Acc: 83.600\n",
      "Epoch 037: | Loss: 0.43213 | Acc: 81.800\n",
      "Epoch 038: | Loss: 0.41704 | Acc: 82.500\n",
      "Epoch 039: | Loss: 0.42428 | Acc: 81.900\n",
      "Epoch 040: | Loss: 0.40004 | Acc: 83.600\n"
     ]
    }
   ],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)  \n",
    "    return acc\n",
    "\n",
    "model.train()\n",
    "EPOCHS = 40\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "#         print (X_batch)\n",
    "        y_pred = model(X_batch)\n",
    "#         print (y_pred)\n",
    "#         print(y_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        acc = binary_acc(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee45860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:18.886357Z",
     "iopub.status.busy": "2022-05-30T14:32:18.885366Z",
     "iopub.status.idle": "2022-05-30T14:32:18.991280Z",
     "shell.execute_reply": "2022-05-30T14:32:18.990095Z"
    },
    "papermill": {
     "duration": 0.116037,
     "end_time": "2022-05-30T14:32:18.993616",
     "exception": false,
     "start_time": "2022-05-30T14:32:18.877579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "# print(model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "#         print(X_batch)\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b802b09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:19.008662Z",
     "iopub.status.busy": "2022-05-30T14:32:19.008135Z",
     "iopub.status.idle": "2022-05-30T14:32:19.022076Z",
     "shell.execute_reply": "2022-05-30T14:32:19.021109Z"
    },
    "papermill": {
     "duration": 0.025769,
     "end_time": "2022-05-30T14:32:19.026034",
     "exception": false,
     "start_time": "2022-05-30T14:32:19.000265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       187\n",
      "           1       0.77      0.67      0.71       108\n",
      "\n",
      "    accuracy                           0.80       295\n",
      "   macro avg       0.79      0.77      0.78       295\n",
      "weighted avg       0.80      0.80      0.80       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_list)\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e3cad12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:19.041927Z",
     "iopub.status.busy": "2022-05-30T14:32:19.041396Z",
     "iopub.status.idle": "2022-05-30T14:32:19.049170Z",
     "shell.execute_reply": "2022-05-30T14:32:19.048196Z"
    },
    "papermill": {
     "duration": 0.017721,
     "end_time": "2022-05-30T14:32:19.051288",
     "exception": false,
     "start_time": "2022-05-30T14:32:19.033567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def testdata_processing(dataset):\n",
    "#     print(dataset)\n",
    "    data = dataset.drop(['PassengerId','Name','Ticket','Cabin','Embarked'], axis = 1)\n",
    "    data['Sex'].replace({'male':0, 'female':1}, inplace = True)\n",
    "    data['Age'].fillna(data['Age'].median(), inplace = True)\n",
    "    data['Fare'].fillna(data['Age'].mean(), inplace = True)\n",
    "    print(data.isna().any())\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1d9062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:19.066118Z",
     "iopub.status.busy": "2022-05-30T14:32:19.065707Z",
     "iopub.status.idle": "2022-05-30T14:32:19.080919Z",
     "shell.execute_reply": "2022-05-30T14:32:19.080214Z"
    },
    "papermill": {
     "duration": 0.025146,
     "end_time": "2022-05-30T14:32:19.083056",
     "exception": false,
     "start_time": "2022-05-30T14:32:19.057910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass    False\n",
      "Sex       False\n",
      "Age       False\n",
      "SibSp     False\n",
      "Parch     False\n",
      "Fare      False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "data = None\n",
    "data = testdata_processing(dataset['test'])\n",
    "# print(data)\n",
    "val_data = TestData(torch.FloatTensor(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4dc361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:19.097696Z",
     "iopub.status.busy": "2022-05-30T14:32:19.097282Z",
     "iopub.status.idle": "2022-05-30T14:32:19.102242Z",
     "shell.execute_reply": "2022-05-30T14:32:19.101378Z"
    },
    "papermill": {
     "duration": 0.014579,
     "end_time": "2022-05-30T14:32:19.104255",
     "exception": false,
     "start_time": "2022-05-30T14:32:19.089676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.TestData object at 0x7f076dd74050>\n"
     ]
    }
   ],
   "source": [
    "print(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f9aadf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:19.119516Z",
     "iopub.status.busy": "2022-05-30T14:32:19.119085Z",
     "iopub.status.idle": "2022-05-30T14:32:19.124082Z",
     "shell.execute_reply": "2022-05-30T14:32:19.122943Z"
    },
    "papermill": {
     "duration": 0.014958,
     "end_time": "2022-05-30T14:32:19.126125",
     "exception": false,
     "start_time": "2022-05-30T14:32:19.111167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_loader = DataLoader(dataset = val_data, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cabbaf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:19.141342Z",
     "iopub.status.busy": "2022-05-30T14:32:19.140943Z",
     "iopub.status.idle": "2022-05-30T14:32:19.286418Z",
     "shell.execute_reply": "2022-05-30T14:32:19.285158Z"
    },
    "papermill": {
     "duration": 0.155499,
     "end_time": "2022-05-30T14:32:19.288648",
     "exception": false,
     "start_time": "2022-05-30T14:32:19.133149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "# print(model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in val_loader:\n",
    "#         print(X_batch)\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "print(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d38c43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T14:32:19.303672Z",
     "iopub.status.busy": "2022-05-30T14:32:19.303261Z",
     "iopub.status.idle": "2022-05-30T14:32:19.337074Z",
     "shell.execute_reply": "2022-05-30T14:32:19.336019Z"
    },
    "papermill": {
     "duration": 0.043933,
     "end_time": "2022-05-30T14:32:19.339369",
     "exception": false,
     "start_time": "2022-05-30T14:32:19.295436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "data_X = dataset['test']\n",
    "print(type(data_X))\n",
    "data_y = y_pred_list\n",
    "data = []\n",
    "for index, row in data_X.iterrows():\n",
    "    data.append([row['PassengerId'], int(data_y[index])])\n",
    "#     print (row['PassengerId'], int(data_y[index]))\n",
    "df = pd.DataFrame(data, columns = ['PassengerId', 'Survived'])\n",
    "df.to_csv(\"predictions.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6759a96",
   "metadata": {
    "papermill": {
     "duration": 0.006352,
     "end_time": "2022-05-30T14:32:19.352672",
     "exception": false,
     "start_time": "2022-05-30T14:32:19.346320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.75477,
   "end_time": "2022-05-30T14:32:20.281633",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-30T14:32:04.526863",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
